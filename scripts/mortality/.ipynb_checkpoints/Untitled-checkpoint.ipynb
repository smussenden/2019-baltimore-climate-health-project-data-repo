{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "For each year, parse the pdf manual, then use that information to\n",
    "unpack the fixed-width data file.\n",
    "\n",
    "Source data files can be found here:\n",
    "https://www.cdc.gov/nchs/data_access/vitalstatsonline.htm#Mortality_Multiple\n",
    "\n",
    "Passes basic tests for 2005-2015. Untested on earlier years.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import re\n",
    "import tabula  # https://github.com/chezou/tabula-py, requires JDK8 install\n",
    "from collections import defaultdict, OrderedDict\n",
    "from slugify import slugify\n",
    "from string import digits, whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS_TO_PROCESS = 2015\n",
    "INPUT_DATA_DIR = \"raw_data\"\n",
    "EXPORT_DATA_DIR = \"cleaned_data\"\n",
    "FIELDS_TO_DROP = {'detail_age'}\n",
    "FIELD_MAP_PATTERNS_TO_DROP = [r'(?i)(_|\\s)condition']\n",
    "MANUALS_DIR = \"file_definitions\"\n",
    "GIBBERISH_SEPARATOR = '>!^@^!<'  # should not match anything\n",
    "UNCODED_FIELDS_TO_KEEP = {r'(?i)ICD_Code.*'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = {\n",
    "    2015: 'VS15MORT.DUSMCPUB'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATHS = {\n",
    "    2015: 'multiple_cause_record_layout_2015.pdf'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detail_Age_Type, Detail_Age are malformed, require special handling\n",
    "HARDCODED_CODES = {x:\n",
    "    {'detail_age_type': {1: 'Years', 2: 'Months', 4: 'Days', 5: 'Hours', 6: 'Minutes', 9: pd.np.nan}}\n",
    "    for x in range(2005, 2016)}\n",
    "\n",
    "HARDCODED_LOCATIONS = {x: {'detail_age_type': (69, 70), 'detail_age': (70, 73)}\n",
    "    for x in range(2005, 2016)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d81af72125e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mprocess_all_years\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-d81af72125e7>\u001b[0m in \u001b[0;36mprocess_all_years\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_all_years\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mYEARS_TO_PROCESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mprocess_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def extract_pdf_section(path, sep=',', pages=None):\n",
    "    # May log several warnings that can be ignored like:\n",
    "    # `org.apache.pdfbox.util.PDFStreamEngine processOperator INFO: unsupported/disabled operation`.\n",
    "    full_path = os.path.join(MANUALS_DIR, path)\n",
    "    try:\n",
    "        return tabula.read_pdf(full_path, guess=False, pages=pages,\n",
    "            pandas_options={'dtype': object, 'header': None, 'sep': sep, 'engine': 'python'})\n",
    "    except pd.errors.ParserError:\n",
    "        pdb.set_trace()\n",
    "\n",
    "\n",
    "def extract_separate_pdf_pages(path):\n",
    "    # distinct function as passing any sep at all causes an error with multiple_tables\n",
    "    full_path = os.path.join(MANUALS_DIR, path)\n",
    "    try:\n",
    "        return tabula.read_pdf(full_path, multiple_tables=True, guess=False, pages='all',\n",
    "            pandas_options={'dtype': object, 'header': None})\n",
    "    except pd.errors.ParserError:\n",
    "        pdb.set_trace()\n",
    "\n",
    "\n",
    "def is_column_definition_page(page_data, year):\n",
    "    # can't just pop the first column definition page; earlier years\n",
    "    # do actually have different format that won't generate false positive\n",
    "    clean_page_data = page_data.dropna(how='all').copy()\n",
    "    cleaned_first_line = str(clean_page_data.iloc[0, -1]).strip()\n",
    "    starts_with_year = cleaned_first_line.endswith(str(year))\n",
    "    cleaned_second_line = str(clean_page_data.iloc[1, -1]).strip()\n",
    "    # spacing is one of the main things tabula fails on\n",
    "    # have to assume zero or multiple spaces where one normally exists\n",
    "    follows_with_title = bool(safe_re_search(r'(?i)Mortality\\s*Multiple\\s*Cause-of-Death\\s*Public\\s*Use\\s*Record$',\n",
    "        cleaned_second_line))\n",
    "    cleaned_third_line = str(clean_page_data.iloc[2, -1]).strip()\n",
    "    is_summary_page = bool(safe_re_search(r'(?i)List\\s*of\\s*File\\s*Data\\s*Elements\\s*and\\s*Tape\\s*Locations',\n",
    "        cleaned_third_line))\n",
    "    return starts_with_year and follows_with_title and not is_summary_page\n",
    "\n",
    "\n",
    "def is_icd_ten_group_page(page_data):\n",
    "    # matches text like 'Tenth Revision 130 Selected Causes of Infant Death Adapted for use by DVS'\n",
    "    cleaned_first_line = str(page_data.iloc[0, 0]).strip()\n",
    "    return bool(safe_re_search(r'(?i)Tenth\\s*Revision\\s*\\d{2,3}\\s*Selected\\s*Causes\\s*of\\s*(\\w+\\s*)?Death', cleaned_first_line))\n",
    "\n",
    "\n",
    "def identify_section_pages(path, year):\n",
    "    # get page numbers for each section.\n",
    "    # need 2 passes as multiple_tables does not support the 'sep' option\n",
    "    column_definition_pages = []\n",
    "    icd_group_pages = []\n",
    "    pages = extract_separate_pdf_pages(path)\n",
    "    have_seen_icd_pages = False\n",
    "    for page_num, page_data in enumerate(pages):\n",
    "        if is_column_definition_page(page_data, year):\n",
    "            column_definition_pages.append(page_num + 1)\n",
    "        elif is_icd_ten_group_page(page_data):\n",
    "            icd_group_pages.append(page_num + 1)\n",
    "            have_seen_icd_pages = True\n",
    "        elif have_seen_icd_pages:\n",
    "            # abort after end of icd to avoid pulling territorial column_definition_pages\n",
    "            break\n",
    "    return column_definition_pages, icd_group_pages\n",
    "\n",
    "\n",
    "def extract_pdf(year):\n",
    "    pdf_path = PDF_PATHS[year]\n",
    "    column_definition_pages, icd_group_pages = identify_section_pages(pdf_path, year)\n",
    "    results = [extract_pdf_section(pdf_path, pages=column_definition_pages)]\n",
    "    # gibberish separator ensures a section is read in as 1 column per page\n",
    "    results.append(extract_pdf_section(pdf_path, pages=icd_group_pages, sep=GIBBERISH_SEPARATOR))\n",
    "    return results\n",
    "\n",
    "\n",
    "def safe_re_search(pattern, text):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    return re.search(pattern, text)\n",
    "\n",
    "\n",
    "def is_column_location_row(location_text):\n",
    "    return bool(safe_re_search(r'^\\d{1,5}(-\\d{1,5})?', location_text))\n",
    "\n",
    "\n",
    "def is_encoding_row(code_text):\n",
    "    return bool(safe_re_search(r'\\.{3}.+', code_text))\n",
    "\n",
    "\n",
    "def is_continuation(field_text):\n",
    "    return bool(safe_re_search(r'(?i)\\s*-\\s*Con\\.$', field_text))\n",
    "\n",
    "\n",
    "def is_reserved_position(field_text):\n",
    "    return bool(safe_re_search(r'(?i)Reserved(\\s*Position)?(s)?$', field_text))\n",
    "\n",
    "\n",
    "def is_condition_entry(field_text):\n",
    "    return bool(safe_re_search(r'(?i)^\\d{1,2}.+\\s*Condition$', field_text))\n",
    "\n",
    "\n",
    "def is_condition_header(field_text):\n",
    "    return bool(safe_re_search(r'(?i)^\\w+\\s*-\\s*Axis\\s*Conditions?$', field_text))\n",
    "\n",
    "\n",
    "def is_condition_size(field_text):\n",
    "    return bool(safe_re_search(r'(?i)^Number\\s*of\\s*\\w+\\s*-\\s*Axis\\s*Conditions?$', field_text))\n",
    "\n",
    "\n",
    "def check_for_duplicate_rows(df):\n",
    "    duplication_filter = (df.field_or_code.duplicated() & df.is_column_loc)\n",
    "    if len(df[duplication_filter]) == 0:\n",
    "        return None\n",
    "    duplicated_fields = list(df[duplication_filter].field_or_code.unique())\n",
    "    raise ValueError(f\"Duplicate field names: {duplicated_fields}\")\n",
    "\n",
    "\n",
    "def add_basic_column_data(df):\n",
    "    try:\n",
    "        df.columns = ['location', 'size', 'field_or_code', 'code_value']\n",
    "    except ValueError:\n",
    "        # some pdfs get parsed with only 3 columns. Known issue with 2009 & earlier, exact num unclear\n",
    "        df.columns = ['location', 'size', 'field_or_code']\n",
    "        df['code_value'] = df['field_or_code'].apply(lambda x:\n",
    "            str(x)[str(x).find('...'):].strip() if str(x).find('...') >= 0 else pd.np.nan)\n",
    "        df['field_or_code'] = df['field_or_code'].apply(lambda x:\n",
    "            str(x)[:str(x).find('...')].strip() if str(x).find('...') >= 0 else x)\n",
    "\n",
    "    # tabula errors can introduce quotes. None exist in pdfs checked by hand.\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: str(x).strip(whitespace + '\"') if not pd.isnull(x) else pd.np.nan)\n",
    "    df['is_column_loc'] = (df['location'].apply(is_column_location_row) &\n",
    "        ~df['size'].apply(pd.isnull))\n",
    "    df['is_encoding'] = (df['code_value'].apply(is_encoding_row) &\n",
    "        ~df['field_or_code'].apply(pd.isnull) & df['size'].apply(pd.isnull))\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_condition_names(df):\n",
    "    # some condition field names can end up in the code_values column\n",
    "    # these are migrated over\n",
    "    malformed_field_filter = (df.is_column_loc & df.field_or_code.isnull())\n",
    "    df.loc[malformed_field_filter, 'field_or_code'] = df[malformed_field_filter].code_value\n",
    "    df['condition_entry'] = (df['field_or_code'].apply(is_condition_entry) & df['is_column_loc'])\n",
    "    df['condition_type'] = (df['field_or_code'].apply(is_condition_header) & df['is_column_loc'])\n",
    "    df['condition_type'] = df.apply(\n",
    "        lambda row: row['field_or_code'].split('-')[0].strip().title()\n",
    "        if row['condition_type'] else None, axis=1)\n",
    "    df['condition_type'].fillna(method='ffill', inplace=True)\n",
    "    df = df[~df['field_or_code'].apply(is_condition_header)].copy()\n",
    "    df['condition_number'] = df.apply(\n",
    "        lambda row: ''.join([i for i in row['field_or_code'] if i.isnumeric()])\n",
    "        if row['condition_entry'] else None, axis=1\n",
    "                                     )\n",
    "    df.loc[df['condition_entry'], 'field_or_code'] = df[df['condition_entry']].apply(\n",
    "            lambda row: f'{row[\"condition_type\"]}_condition_{row[\"condition_number\"]}', axis=1)\n",
    "    df.loc[df['condition_entry'], 'code_value'] = pd.np.nan\n",
    "    df.loc[df['condition_entry'], 'is_column_loc'] = False\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_useless_rows(df):\n",
    "    df = df[df.is_column_loc | df.is_encoding].copy()\n",
    "    df = df[~(df['field_or_code'].apply(is_continuation) & df['is_column_loc'])].copy()\n",
    "    df = df[~df['field_or_code'].apply(lambda x: str(x).startswith('Reserved'))].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    df['location'].fillna(method='ffill', inplace=True)\n",
    "    df['size'].fillna(method='ffill', inplace=True)\n",
    "    df.fillna(value='', inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tidy_data(df):\n",
    "    df['code_value'] = df['code_value'].apply(lambda x:\n",
    "        str(x)[str(x).find('...') + len('...'):].strip() if str(x).find('...') >= 0 else pd.np.nan)\n",
    "    # documentation uses 1 based indexing, we need 0 based\n",
    "    df['location_start'] = df['location'].apply(lambda x: int(x.split('-')[0]) - 1)\n",
    "    # first page of results has non-unique code keys for US vs territorial datasets\n",
    "    # retaining first keeps US keys\n",
    "    df.drop_duplicates(subset={'location', 'size', 'field_or_code'}, inplace=True)\n",
    "    df['size'] = df['size'].apply(int)\n",
    "    df['location_end'] = df.apply(\n",
    "            lambda row: row['location_start'] + row['size'], axis=1)\n",
    "    df['field'] = df.apply(lambda row: row['field_or_code']\n",
    "        if any([row['is_column_loc'], row['condition_entry']]) else pd.np.nan, axis=1)\n",
    "    df['field'].fillna(method='ffill', inplace=True)\n",
    "    df['field'] = df['field'].apply(lambda x: slugify(x).replace('-', '_'))\n",
    "    df.rename(columns={'field_or_code': 'code'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def populate_field_data(df, year):\n",
    "    df['field_to_keep'] = df['field'].apply(lambda x:\n",
    "        any([bool(safe_re_search(pattern, x)) for pattern in UNCODED_FIELDS_TO_KEEP]))\n",
    "    df = df[~df['is_column_loc'] | df['field_to_keep']].copy()\n",
    "    df = df[~df['field'].isin(FIELDS_TO_DROP)].copy()\n",
    "    # enforcing title case to remove source of inconsistencies across years\n",
    "    df['code'] = df['code'].apply(str.title)\n",
    "    field_codes = defaultdict(dict)\n",
    "    df.apply(lambda row:\n",
    "        field_codes[row['field']].update({row['code']: row['code_value']}), axis=1)\n",
    "    field_codes.update(HARDCODED_CODES[year])\n",
    "    field_codes = {k: v for k, v in field_codes.items()\n",
    "        if not any([re.search(pattern, k) for pattern in FIELD_MAP_PATTERNS_TO_DROP])}\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.sort_values(by=['location_start'], inplace=True)\n",
    "    df['location'] = df.apply(\n",
    "        lambda row: (row['location_start'], row['location_end']), axis=1)\n",
    "    df.drop_duplicates(subset=['location'], inplace=True)\n",
    "    field_locations = OrderedDict()\n",
    "    df.apply(lambda row: field_locations.update({row['field']: row['location']}), axis=1)\n",
    "    field_locations.update(HARDCODED_LOCATIONS[year])\n",
    "    field_locations = OrderedDict(sorted(field_locations.items(), key=lambda x: x[1][0]))\n",
    "    return field_locations, dict(field_codes), df\n",
    "\n",
    "\n",
    "def parse_pdf_data(df, year):\n",
    "    df = add_basic_column_data(df)\n",
    "    df = drop_useless_rows(df)\n",
    "    df = update_condition_names(df)\n",
    "    check_for_duplicate_rows(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = tidy_data(df)\n",
    "    return populate_field_data(df, year)\n",
    "\n",
    "\n",
    "def read_dataset(year, field_locations):\n",
    "    full_path = os.path.join(INPUT_DATA_DIR, DATA_PATHS[year])\n",
    "    return pd.read_fwf(full_path, colspecs=list(field_locations.values()),\n",
    "        names=list(field_locations.keys()), header=None, dtype=object)\n",
    "\n",
    "\n",
    "def validate_data_types(df, year):\n",
    "    # since we don't know the complete list of columns to expect, we only\n",
    "    # validate a small sample that definitely ought exist with consistent formats\n",
    "    if not all([\n",
    "        set(df['autopsy'].apply(str.upper).unique()) == {'Y', 'N', 'U'},\n",
    "        set(df['sex'].unique()) == {'M', 'F'},\n",
    "        all(df['detail_age_type'].apply(str.isnumeric)),\n",
    "        df['current_data_year'].unique().tolist() == [str(year)],\n",
    "        len([x for x in df.columns if 'record' in x]) >= 20,\n",
    "        len([x for x in df.columns if 'entity' in x]) >= 20,\n",
    "               ]):\n",
    "        pdb.set_trace()\n",
    "        raise ValueError(f\"Invalid datatype found in {year} data\")\n",
    "\n",
    "\n",
    "def safe_first_re_match(pattern, text):\n",
    "    match = safe_re_search(pattern, text)\n",
    "    if match:\n",
    "        return match[0]\n",
    "\n",
    "\n",
    "def extract_cause_title(text):\n",
    "    return safe_first_re_match(r'(?i)\\b\\d{1,4}\\s*selected\\s*causes\\b', text)\n",
    "\n",
    "\n",
    "def is_idc_data_header(text):\n",
    "    return bool(safe_re_search(r'(?i)^recodetsexagecause', re.sub('\\s', '', text)))\n",
    "\n",
    "\n",
    "def extract_recode(text):\n",
    "    return safe_first_re_match(r'^\\d{3}', text)\n",
    "\n",
    "\n",
    "def remove_filler_rows(df):\n",
    "    df['is_header'] = df['text'].apply(is_idc_data_header)\n",
    "    cause_title_indexes = list(df[df.is_cause_title].index)\n",
    "    header_indexes = list(df[df.is_header].index)\n",
    "    filler_indexes = [x for x in zip(cause_title_indexes, header_indexes)]\n",
    "    for min_idx, max_idx in filler_indexes:\n",
    "        df = df[(df.index < min_idx) | (df.index > max_idx)].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_ICD_codes(icd_codes):\n",
    "    # codes should be form complete sequence within each group\n",
    "    for cause_title, code_group in icd_codes.items():\n",
    "        int_icd_codes = sorted([int(x) for x in code_group.keys()])\n",
    "        expected_code_range = [i for i in range(min(int_icd_codes), max(int_icd_codes) + 1)]\n",
    "        if int_icd_codes != expected_code_range:\n",
    "            pdb.set_trace()\n",
    "            raise ValueError(f'ICD codes for {cause_title} do not form complete sequence')\n",
    "\n",
    "\n",
    "def read_ICD_codes(df):\n",
    "    df.columns = ['text']\n",
    "    df.dropna(inplace=True)\n",
    "    # some lines end up improperly quote wrapped due to tabula error\n",
    "    df['text'] = df[~df['text'].isnull()].copy()['text'].apply(\n",
    "        lambda x: x.strip('\"') if not pd.isnull(x) else pd.np.nan)\n",
    "    df['cause_title'] = df['text'].apply(extract_cause_title)\n",
    "    df['is_cause_title'] = ~df['cause_title'].isnull()\n",
    "    df['cause_title'].fillna(method='ffill', inplace=True)\n",
    "    df = remove_filler_rows(df)\n",
    "    df['recode'] = df['text'].apply(extract_recode)\n",
    "    df['recode'].fillna(method='ffill', inplace=True)\n",
    "    # very first row can still be blank; can be safely dropped\n",
    "    df = df.dropna(subset=['text', 'recode'], how='any').copy()\n",
    "    df['text'] = (df[['text', 'recode', 'cause_title']].groupby(['cause_title', 'recode'])\n",
    "        .transform(lambda x: ' '.join(x)))\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # tabula fails to retain spacing; we cannot recover subtotal/gender/age codes\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip(digits + whitespace).strip())\n",
    "    icd_codes = {title: dict() for title in df['cause_title'].unique()}\n",
    "    df.apply(lambda row: icd_codes[row['cause_title']].update({row['recode']: row['text']}), axis=1)\n",
    "    validate_ICD_codes(icd_codes)\n",
    "    return icd_codes\n",
    "\n",
    "\n",
    "def update_field_map(field_map, icd_codes):\n",
    "    for code_group, icd_values in icd_codes.items():\n",
    "        group_number = safe_first_re_match('\\d{1,4}', code_group)\n",
    "        matching_pattern = f'(?i){group_number}.+Cause_Recode$'\n",
    "        field_map_matching_key = [x for x in field_map.keys() if safe_re_search(matching_pattern, x)][0]\n",
    "        field_map[field_map_matching_key] = icd_values\n",
    "    return field_map\n",
    "\n",
    "\n",
    "def export_code_maps(field_map, year):\n",
    "    with open(os.path.join(EXPORT_DATA_DIR, f'{year}_codes.json'), 'w+') as f_open:\n",
    "        json.dump(field_map, f_open)\n",
    "\n",
    "\n",
    "def process_year(year):\n",
    "    print(f\"Unpacking pdf for {year}\")\n",
    "    documentation_data = extract_pdf(year)\n",
    "    field_locations, field_map, df = parse_pdf_data(documentation_data[0], year)\n",
    "    print(f\"Parsing fixed with file for {year}\")\n",
    "    df = read_dataset(year, field_locations)\n",
    "    validate_data_types(df, year)\n",
    "    df.to_csv(os.path.join(EXPORT_DATA_DIR, f'{year}_data.csv'), index=False)\n",
    "    print(f\"Exporting column code mappings\")\n",
    "    icd_codes = read_ICD_codes(documentation_data[1])\n",
    "    field_map = update_field_map(field_map, icd_codes)\n",
    "    export_code_maps(field_map, year)\n",
    "    print(f\"Finished {year}\")\n",
    "\n",
    "\n",
    "def process_all_years():\n",
    "    for year in YEARS_TO_PROCESS:\n",
    "        process_year(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
